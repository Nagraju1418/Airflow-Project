from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator
from datetime import timedelta, datetime
import pandas as pd
from airflow.operators.python_operator import EmailOperator


default_args={
        'owner':’Nagaraju’,
        'start_date':datetime(2023,4,5),
        'retries':4,
        'retry_delay':timedelta(minutes=5)
        }

Timberland_stock_analysis= DAG('Timberland_stock_analysis',
          default_args=default_args,
          description='Timberland_stock_analysis',
          schedule_interval='* * * * *',
          catchup=False,
          tags=['example,helloworld']
          )        

# Task 1: Dummy Operator to start the task
task1 = DummyOperator(task_id='star_task', dag=Timberland_stock_analysis)

# Task 2: Run Spark job to read CSV and send output file
def run_spark_job():
    # Read CSV file using Spark and perform required processing
    # Save the output file to /root/airflow/outputfiles/

    # Example Spark job code:
    from pyspark.sql import SparkSession
    
    spark = SparkSession.builder.appName("airflow_pipeline").getOrCreate()
    
    # Read CSV file
    csv_data = spark.read.option("header" , True).csv("/root/airflow/inputfiles/timberland_stock.csv")
    csv_data.createOrReplaceTempView("airflow table")
    Peak_High = spark.sql("select Date from airflow table where High = (select max(High) from mytable)")
    Mean_Column = spark.sql("select avg(Close) as mean_of_column from airflow table")
    Max_Column = spark.sql("select max(Volume) as max_of_volume from airflow table")
    Min_of_Volume_Column = spark.sql("select min(Volume) as min_of_volume from airflow table")
    No_Of_days = spark.sql("SELECT COUNT(*) AS count_lower_than_60 FROM airflow table WHERE Close < 60")
    percentage = spark.sql("SELECT (COUNT(CASE WHEN High > 80 THEN 1 END) / COUNT(*)) * 100 AS percentage_high_above_80 FROM airflow table")
    Correlation  = spark.sql("SELECT corr(High, Volume) AS correlation FROM airflow table")
    Max_Year = spark.sql("""SELECT YEAR(Date) AS Year, MAX(CAST(High AS DOUBLE)) AS max_high FROM airflow table GROUP BY Year ORDER BY Year""")
    Avg_Month = spark.sql("""SELECT YEAR(Date) AS Year, MONTH(Date) AS Month, AVG(Close) AS AvgClose FROM airflow table GROUP BY Year, Month ORDER BY Year, Month""")

    # Save output file
    Peak_High.write.csv('/root/airflow/outputfiles/Peak_High_Price_Date.csv', mode='overwrite', header=True)

    Mean_Column.write.csv('/root/airflow/outputfiles/Mean_Of_Close_Column.csv', mode='overwrite', header=True)

    Max_Column.write.csv('/root/airflow/outputfiles/Max_of_Volume_Column.csv', mode='overwrite', header=True)

    Min_of_Volume_Column.write.csv('/root/airflow/outputfiles/Min_of_Volume_Column.csv', mode='overwrite', header=True)

    No_Of_days.write.csv('/root/airflow/outputfiles/No_Of_days.csv', mode='overwrite', header=True)

    percentage.write.csv('/root/airflow/outputfiles/percentage.csv', mode='overwrite', header=True)

    Correlation.write.csv('/root/airflow/outputfiles/Pearson_Correlation.csv', mode='overwrite', header=True)

    Max_Year.write.csv('/root/airflow/outputfiles/Max_High_Year.csv', mode='overwrite', header=True)

    Avg_Month.write.csv('/root/airflow/outputfiles/Avg_Close_For_Each_Month.csv', mode='overwrite', header=True)


task2 = PythonOperator(
    task_id='run_spark_job',
    python_callable=run_spark_job,
    dag=Timberland_stock_analysis
)

# Task 3: Dummy Operator to end the task
task3 = DummyOperator(task_id='end_task',dag=Timberland_stock_analysis)

# Task 4 : Email Operator
Task4=  EmailOperator(
        task_id = "email_task",
        to=['nagaraj141814@gmail.com'],
        subject = "Airflow Alert!",
        html_content="<i>Msg from Airflow -- outputfile is generated</i>",
        dag= Timberland_stock_analysis
      )



# Define task dependencies
task1 >> task2 >> task3 >> task4